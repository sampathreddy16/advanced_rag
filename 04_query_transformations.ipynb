{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformations for Improved Retrieval in RAG Systems\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code implements three query transformation techniques to enhance the retrieval process in Retrieval-Augmented Generation (RAG) systems:\n",
    "\n",
    "1. Query Rewriting\n",
    "2. Step-back Prompting\n",
    "3. Sub-query Decomposition\n",
    "\n",
    "Each technique aims to improve the relevance and comprehensiveness of retrieved information by modifying or expanding the original query.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "RAG systems often face challenges in retrieving the most relevant information, especially when dealing with complex or ambiguous queries. These query transformation techniques address this issue by reformulating queries to better match relevant documents or to retrieve more comprehensive information.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Query Rewriting: Reformulates queries to be more specific and detailed.\n",
    "2. Step-back Prompting: Generates broader queries for better context retrieval.\n",
    "3. Sub-query Decomposition: Breaks down complex queries into simpler sub-queries.\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### 1. Query Rewriting\n",
    "\n",
    "- **Purpose**: To make queries more specific and detailed, improving the likelihood of retrieving relevant information.\n",
    "- **Implementation**:\n",
    "  - Uses a GPT-4 model with a custom prompt template.\n",
    "  - Takes the original query and reformulates it to be more specific and detailed.\n",
    "\n",
    "### 2. Step-back Prompting\n",
    "\n",
    "- **Purpose**: To generate broader, more general queries that can help retrieve relevant background information.\n",
    "- **Implementation**:\n",
    "  - Uses a GPT-4 model with a custom prompt template.\n",
    "  - Takes the original query and generates a more general \"step-back\" query.\n",
    "\n",
    "### 3. Sub-query Decomposition\n",
    "\n",
    "- **Purpose**: To break down complex queries into simpler sub-queries for more comprehensive information retrieval.\n",
    "- **Implementation**:\n",
    "  - Uses a GPT-4 model with a custom prompt template.\n",
    "  - Decomposes the original query into 2-4 simpler sub-queries.\n",
    "\n",
    "## Benefits of these Approaches\n",
    "\n",
    "1. **Improved Relevance**: Query rewriting helps in retrieving more specific and relevant information.\n",
    "2. **Better Context**: Step-back prompting allows for retrieval of broader context and background information.\n",
    "3. **Comprehensive Results**: Sub-query decomposition enables retrieval of information that covers different aspects of a complex query.\n",
    "4. **Flexibility**: Each technique can be used independently or in combination, depending on the specific use case.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- All techniques use OpenAI's GPT-4 model for query transformation.\n",
    "- Custom prompt templates are used to guide the model in generating appropriate transformations.\n",
    "- The code provides separate functions for each transformation technique, allowing for easy integration into existing RAG systems.\n",
    "\n",
    "## Example Use Case\n",
    "\n",
    "The code demonstrates each technique using the example query:\n",
    "\"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "- **Query Rewriting** expands this to include specific aspects like temperature changes and biodiversity.\n",
    "- **Step-back Prompting** generalizes it to \"What are the general effects of climate change?\"\n",
    "- **Sub-query Decomposition** breaks it down into questions about biodiversity, oceans, weather patterns, and terrestrial environments.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "These query transformation techniques offer powerful ways to enhance the retrieval capabilities of RAG systems. By reformulating queries in various ways, they can significantly improve the relevance, context, and comprehensiveness of retrieved information. These methods are particularly valuable in domains where queries can be complex or multifaceted, such as scientific research, legal analysis, or comprehensive fact-finding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!uv pip install langchain langchain-openai langchain-core python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Query Rewriting: Reformulating queries to improve retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_write_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "# Create a prompt template for query rewriting\n",
    "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Rewritten query:\"\"\"\n",
    "\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=query_rewrite_template\n",
    ")\n",
    "\n",
    "# Create an LLMChain for query rewriting\n",
    "query_rewriter = query_rewrite_prompt | re_write_llm\n",
    "\n",
    "def rewrite_query(original_query):\n",
    "    \"\"\"\n",
    "    Rewrite the original query to improve retrieval.\n",
    "    \n",
    "    Args:\n",
    "    original_query (str): The original user query\n",
    "    \n",
    "    Returns:\n",
    "    str: The rewritten query\n",
    "    \"\"\"\n",
    "    response = query_rewriter.invoke(original_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate on a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What are the impacts of climate change on the environment?\n",
      "\n",
      "Rewritten query: How does climate change affect various aspects of the environment, such as biodiversity, ecosystems, weather patterns, and sea levels?\n"
     ]
    }
   ],
   "source": [
    "# example query over the understanding climate change dataset\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nRewritten query:\", rewritten_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Step-back Prompting: Generating broader queries for better context retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "\n",
    "# Create a prompt template for step-back prompting\n",
    "step_back_template = \"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Step-back query:\"\"\"\n",
    "\n",
    "step_back_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=step_back_template\n",
    ")\n",
    "\n",
    "# Create an LLMChain for step-back prompting\n",
    "step_back_chain = step_back_prompt | step_back_llm\n",
    "\n",
    "def generate_step_back_query(original_query):\n",
    "    \"\"\"\n",
    "    Generate a step-back query to retrieve broader context.\n",
    "    \n",
    "    Args:\n",
    "    original_query (str): The original user query\n",
    "    \n",
    "    Returns:\n",
    "    str: The step-back query\n",
    "    \"\"\"\n",
    "    response = step_back_chain.invoke(original_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate on a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What are the impacts of climate change on the environment?\n",
      "\n",
      "Step-back query: What are the general effects of climate change on natural systems and ecosystems?\n"
     ]
    }
   ],
   "source": [
    "# example query over the understanding climate change dataset\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nStep-back query:\", step_back_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Sub-query Decomposition: Breaking complex queries into simpler sub-queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_query_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "# Create a prompt template for sub-query decomposition\n",
    "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\"\"\"\n",
    "\n",
    "\n",
    "subquery_decomposition_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=subquery_decomposition_template\n",
    ")\n",
    "\n",
    "# Create an LLMChain for sub-query decomposition\n",
    "subquery_decomposer_chain = subquery_decomposition_prompt | sub_query_llm\n",
    "\n",
    "def decompose_query(original_query: str):\n",
    "    \"\"\"\n",
    "    Decompose the original query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "    original_query (str): The original complex query\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    response = subquery_decomposer_chain.invoke(original_query).content\n",
    "    sub_queries = [q.strip() for q in response.split('\\n') if q.strip() and not q.strip().startswith('Sub-queries:')]\n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate on a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sub-queries:\n",
      "1. How does climate change affect weather patterns and extreme weather events?\n",
      "2. What are the impacts of climate change on ecosystems and wildlife habitats?\n",
      "3. How does climate change influence sea levels and coastal areas?\n",
      "4. What are the effects of climate change on freshwater resources and availability?\n"
     ]
    }
   ],
   "source": [
    "# example query over the understanding climate change dataset\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "sub_queries = decompose_query(original_query)\n",
    "print(\"\\nSub-queries:\")\n",
    "for i, sub_query in enumerate(sub_queries, 1):\n",
    "    print(sub_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformations in RAG\n",
    "\n",
    "## Query Rewriting/Refinement\n",
    "- **Query expansion** - adding related terms or synonyms\n",
    "- **Query decomposition** - breaking complex queries into sub-queries\n",
    "- **Query simplification** - making queries more concise\n",
    "- **Query rephrasing** - restating the question in different ways\n",
    "\n",
    "## Multi-Query Generation\n",
    "- Generating multiple variations of the same query\n",
    "- Creating parallel queries from different perspectives\n",
    "- Producing diverse phrasings for better coverage\n",
    "\n",
    "## Step-Back Prompting\n",
    "- Abstracting the query to a higher-level question\n",
    "- Creating broader context queries before specific ones\n",
    "\n",
    "## HyDE (Hypothetical Document Embeddings)\n",
    "- Generating hypothetical answers/documents\n",
    "- Using these as queries instead of the original question\n",
    "\n",
    "## Query Routing\n",
    "- Classifying query type (factual, analytical, conversational)\n",
    "- Determining which knowledge source to query\n",
    "- Selecting appropriate retrieval strategy\n",
    "\n",
    "## Query Compression\n",
    "- Removing unnecessary words or context\n",
    "- Focusing on key semantic elements\n",
    "\n",
    "## Contextual Query Enhancement\n",
    "- Adding conversation history context\n",
    "- Incorporating user metadata or preferences\n",
    "- Appending temporal or domain-specific context\n",
    "\n",
    "## Query Translation\n",
    "- Converting natural language to structured queries (e.g., SQL, metadata filters)\n",
    "- Transforming to domain-specific terminology\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** These transformations can be used individually or combined depending on your RAG architecture and use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Explore More Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Set your API key here or use environment variable\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Or load from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Query Generation\n",
    "\n",
    "**Purpose:** Generate multiple variations of a query to improve retrieval recall.\n",
    "\n",
    "**Why it matters:** Different phrasings can retrieve different relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_generation(original_query: str, num_queries: int = 3) -> List[str]:\n",
    "    prompt = f\"\"\"You are an AI assistant helping to improve search queries.\n",
    "    \n",
    "Generate {num_queries} different versions of the following question. \n",
    "Each version should:\n",
    "- Ask the same thing but from a different perspective\n",
    "- Use different wording and phrasing\n",
    "- Help retrieve relevant information from a vector database\n",
    "\n",
    "Original question: {original_query}\n",
    "\n",
    "Return ONLY a JSON array of {num_queries} alternative questions, nothing else.\n",
    "Example format: [\"question 1\", \"question 2\", \"question 3\"]\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    queries = json.loads(response.choices[0].message.content)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Query: What are the benefits of using RAG in LLM applications?\n",
      "\n",
      "üîÑ Generated Variations:\n",
      "  1. What advantages does using RAG provide in LLM applications?\n",
      "  2. How does utilizing RAG enhance LLM applications?\n",
      "  3. What positive impacts does the RAG system have on LLM applications?\n"
     ]
    }
   ],
   "source": [
    "# Try it out!\n",
    "original_query = \"What are the benefits of using RAG in LLM applications?\"\n",
    "\n",
    "print(f\"üìù Original Query: {original_query}\\n\")\n",
    "variations = multi_query_generation(original_query, num_queries=3)\n",
    "\n",
    "print(\"üîÑ Generated Variations:\")\n",
    "for i, var in enumerate(variations, 1):\n",
    "    print(f\"  {i}. {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Routing\n",
    "\n",
    "**Purpose:** Route queries to the most appropriate data source(s).\n",
    "\n",
    "**Why it matters:** Different data sources excel at different types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_routing(query: str, available_sources: List[Dict[str, str]]) -> Dict:\n",
    "    sources_text = \"\\n\".join([\n",
    "        f\"{i+1}. {source['name']}: {source['description']}\" \n",
    "        for i, source in enumerate(available_sources)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are a query router. Given a user question and available data sources, \n",
    "select the MOST appropriate source(s) to answer the question.\n",
    "\n",
    "Available Sources:\n",
    "{sources_text}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Return your answer as JSON with this structure:\n",
    "{{\n",
    "    \"selected_sources\": [\"source_name1\", \"source_name2\"],\n",
    "    \"reasoning\": \"brief explanation of why these sources\"\n",
    "}}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Query: How do I deploy a machine learning model to production?\n",
      "\n",
      "üóÇÔ∏è Available Sources: ['ml_docs', 'deployment_guide', 'api_reference', 'case_studies']\n",
      "\n",
      "üéØ Routing Decision:\n",
      "  Selected: ['deployment_guide', 'ml_docs']\n",
      "  Reasoning: The 'deployment_guide' is the most relevant source for information on deploying applications, including machine learning models, to production environments. It provides cloud deployment and DevOps documentation that is essential for this task. The 'ml_docs' source is also selected because it contains tutorials and best practices for machine learning, which may include specific considerations for deploying models effectively.\n"
     ]
    }
   ],
   "source": [
    "# Define available data sources\n",
    "sources = [\n",
    "    {\"name\": \"ml_docs\", \"description\": \"Machine learning tutorials and best practices\"},\n",
    "    {\"name\": \"deployment_guide\", \"description\": \"Cloud deployment and DevOps documentation\"},\n",
    "    {\"name\": \"api_reference\", \"description\": \"API specifications and code examples\"},\n",
    "    {\"name\": \"case_studies\", \"description\": \"Real-world implementation case studies\"}\n",
    "]\n",
    "\n",
    "query = \"How do I deploy a machine learning model to production?\"\n",
    "\n",
    "print(f\"üìù Query: {query}\\n\")\n",
    "print(f\"üóÇÔ∏è Available Sources: {[s['name'] for s in sources]}\\n\")\n",
    "\n",
    "routing_result = query_routing(query, sources)\n",
    "\n",
    "print(\"üéØ Routing Decision:\")\n",
    "print(f\"  Selected: {routing_result['selected_sources']}\")\n",
    "print(f\"  Reasoning: {routing_result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Compression\n",
    "\n",
    "**Purpose:** Remove unnecessary words while preserving search intent.\n",
    "\n",
    "**Why it matters:** Embedding models work better with focused, keyword-rich queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_compression(query: str, conversation_history: List[Dict[str, str]] = None) -> str:\n",
    "    context = \"\"\n",
    "    if conversation_history:\n",
    "        context = \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\" \n",
    "            for msg in conversation_history[-3:]\n",
    "        ])\n",
    "        context = f\"\\nConversation History:\\n{context}\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a query optimization expert. Compress the following query into a concise search query.\n",
    "\n",
    "{context}\n",
    "Current Query: {query}\n",
    "\n",
    "Rules:\n",
    "- Remove filler words and unnecessary context\n",
    "- Keep only the core semantic concepts\n",
    "- Maintain the search intent\n",
    "- Output should be 3-8 words maximum\n",
    "- Focus on keywords that would match relevant documents\n",
    "\n",
    "Return ONLY the compressed query, nothing else.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    compressed_query = response.choices[0].message.content.strip()\n",
    "    return compressed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Verbose Query (182 chars):\n",
      "I was wondering if you could help me understand what the main advantages are of implementing a Retrieval Augmented Generation system compared to just using a standard language model?\n",
      "\n",
      "‚úÇÔ∏è Compressed Query (73 chars):\n",
      "\"Advantages of Retrieval Augmented Generation vs standard language model\"\n",
      "\n",
      "Compression Ratio: 40.1%\n"
     ]
    }
   ],
   "source": [
    "verbose_query = \"I was wondering if you could help me understand what the main advantages are of implementing a Retrieval Augmented Generation system compared to just using a standard language model?\"\n",
    "\n",
    "print(f\"üìù Verbose Query ({len(verbose_query)} chars):\\n{verbose_query}\\n\")\n",
    "\n",
    "compressed = query_compression(verbose_query)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Compressed Query ({len(compressed)} chars):\\n{compressed}\\n\")\n",
    "print(f\"Compression Ratio: {len(compressed)/len(verbose_query)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression with Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What about the implementation details?\n",
      "Compressed: \"RAG systems implementation details\"\n"
     ]
    }
   ],
   "source": [
    "# Test with conversation context\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about RAG systems\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"RAG systems combine retrieval with generation...\"}\n",
    "]\n",
    "\n",
    "query_with_context = \"What about the implementation details?\"\n",
    "\n",
    "compressed_with_context = query_compression(query_with_context, conversation)\n",
    "print(f\"Original: {query_with_context}\")\n",
    "print(f\"Compressed: {compressed_with_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Query Enhancement\n",
    "\n",
    "**Purpose:** Enrich queries with user context and conversation history.\n",
    "\n",
    "**Why it matters:** Personalization leads to more relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_query_enhancement(\n",
    "    query: str, \n",
    "    user_context: Dict[str, str],\n",
    "    conversation_history: List[Dict[str, str]] = None\n",
    ") -> str:\n",
    "    context_text = \"\\n\".join([f\"- {k}: {v}\" for k, v in user_context.items()])\n",
    "    \n",
    "    history_text = \"\"\n",
    "    if conversation_history:\n",
    "        history_text = \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\" \n",
    "            for msg in conversation_history[-3:]\n",
    "        ])\n",
    "    \n",
    "    prompt = f\"\"\"You are enhancing a search query with contextual information.\n",
    "\n",
    "User Context:\n",
    "{context_text}\n",
    "\n",
    "{f\"Recent Conversation:{chr(10)}{history_text}\" if history_text else \"\"}\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Task: Enhance this query by:\n",
    "1. Adding relevant context from user profile\n",
    "2. Incorporating conversation history if relevant\n",
    "3. Making implicit information explicit\n",
    "4. Keeping it natural and search-friendly\n",
    "\n",
    "Return ONLY the enhanced query, nothing else.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    enhanced_query = response.choices[0].message.content.strip()\n",
    "    return enhanced_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Query: What are the best practices?\n",
      "\n",
      "üë§ User Context: {'role': 'Senior Data Engineer', 'experience': '5 years', 'focus_area': 'MLOps and model deployment', 'current_stack': 'Python, Docker, Kubernetes'}\n",
      "\n",
      "‚ú® Enhanced Query: \"What are the best practices for a Senior Data Engineer with 5 years of experience focused on MLOps and model deployment using Python, Docker, and Kubernetes?\"\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the best practices?\"\n",
    "user_context = {\n",
    "    \"role\": \"Senior Data Engineer\",\n",
    "    \"experience\": \"5 years\",\n",
    "    \"focus_area\": \"MLOps and model deployment\",\n",
    "    \"current_stack\": \"Python, Docker, Kubernetes\"\n",
    "}\n",
    "conv_history = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about deploying ML models\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"There are several approaches to model deployment...\"}\n",
    "]\n",
    "\n",
    "print(f\"üìù Original Query: {query}\\n\")\n",
    "print(f\"üë§ User Context: {user_context}\\n\")\n",
    "\n",
    "enhanced = contextual_query_enhancement(query, user_context, conv_history)\n",
    "\n",
    "print(f\"‚ú® Enhanced Query: {enhanced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalize for Different Users\n",
    "Try different user contexts and see how the query changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Scientist: \"How do I optimize deep learning model performance with 2 years of data science experience?\"\n",
      "\n",
      "ML Engineer: What are best practices for optimizing machine learning model performance in production systems for an engineer with 7 years of experience?\n",
      "\n",
      "Research Scientist: What are the best practices for optimizing performance of NLP models in research, considering a decade of experience in the field?\n"
     ]
    }
   ],
   "source": [
    "# Try with different user profiles\n",
    "profiles = [\n",
    "    {\"role\": \"Data Scientist\", \"experience\": \"2 years\", \"focus_area\": \"Deep Learning\"},\n",
    "    {\"role\": \"ML Engineer\", \"experience\": \"7 years\", \"focus_area\": \"Production Systems\"},\n",
    "    {\"role\": \"Research Scientist\", \"experience\": \"10 years\", \"focus_area\": \"NLP Research\"}\n",
    "]\n",
    "\n",
    "query = \"How do I optimize model performance?\"\n",
    "\n",
    "for profile in profiles:\n",
    "    enhanced = contextual_query_enhancement(query, profile)\n",
    "    print(f\"\\n{profile['role']}: {enhanced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation (Natural Language to Filters)\n",
    "\n",
    "**Purpose:** Extract semantic query and metadata filters from natural language.\n",
    "\n",
    "**Why it matters:** Combines semantic search with precise filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_translation_to_filters(query: str, available_filters: Dict[str, List]) -> Dict:\n",
    "    filters_description = \"\\n\".join([\n",
    "        f\"- {field}: {', '.join(map(str, values))}\" \n",
    "        for field, values in available_filters.items()\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are a query translator. Extract the semantic query and metadata filters from a natural language query.\n",
    "\n",
    "Available Metadata Filters:\n",
    "{filters_description}\n",
    "\n",
    "Natural Language Query: {query}\n",
    "\n",
    "Parse this into:\n",
    "1. A semantic search query (the conceptual question)\n",
    "2. Metadata filters (specific attributes to filter on)\n",
    "\n",
    "Return JSON in this format:\n",
    "{{\n",
    "    \"semantic_query\": \"the core question without filter terms\",\n",
    "    \"filters\": {{\n",
    "        \"field_name\": \"value or condition\"\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Only include filters that are explicitly mentioned in the query.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Natural Language Query:\n",
      "Show me research papers about transformers published after 2020 with high citations\n",
      "\n",
      "üîß Available Filters:\n",
      "{\n",
      "  \"year\": [\n",
      "    2018,\n",
      "    2019,\n",
      "    2020,\n",
      "    2021,\n",
      "    2022,\n",
      "    2023,\n",
      "    2024\n",
      "  ],\n",
      "  \"topic\": [\n",
      "    \"transformers\",\n",
      "    \"CNN\",\n",
      "    \"RNN\",\n",
      "    \"attention\",\n",
      "    \"RAG\"\n",
      "  ],\n",
      "  \"citation_count\": [\n",
      "    \"low\",\n",
      "    \"medium\",\n",
      "    \"high\"\n",
      "  ],\n",
      "  \"author\": [\n",
      "    \"various\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "üéØ Translation Result:\n",
      "{\n",
      "  \"semantic_query\": \"research papers about transformers\",\n",
      "  \"filters\": {\n",
      "    \"year\": \"after 2020\",\n",
      "    \"citation_count\": \"high\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"Show me research papers about transformers published after 2020 with high citations\"\n",
    "available_filters = {\n",
    "    \"year\": list(range(2018, 2025)),\n",
    "    \"topic\": [\"transformers\", \"CNN\", \"RNN\", \"attention\", \"RAG\"],\n",
    "    \"citation_count\": [\"low\", \"medium\", \"high\"],\n",
    "    \"author\": [\"various\"]\n",
    "}\n",
    "\n",
    "print(f\"üìù Natural Language Query:\\n{query}\\n\")\n",
    "print(f\"üîß Available Filters:\\n{json.dumps(available_filters, indent=2)}\\n\")\n",
    "\n",
    "translation = query_translation_to_filters(query, available_filters)\n",
    "\n",
    "print(\"üéØ Translation Result:\")\n",
    "print(json.dumps(translation, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-commerce Search\n",
    "Try translating product search queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"semantic_query\": \"Find Apple laptops\",\n",
      "  \"filters\": {\n",
      "    \"rating\": \"4, 5\",\n",
      "    \"price_range\": \"under_1000\",\n",
      "    \"in_stock\": \"yes\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# E-commerce example\n",
    "ecommerce_filters = {\n",
    "    \"category\": [\"electronics\", \"clothing\", \"home\", \"sports\"],\n",
    "    \"price_range\": [\"under_50\", \"50-100\", \"100-500\", \"over_500\"],\n",
    "    \"brand\": [\"Apple\", \"Samsung\", \"Nike\", \"various\"],\n",
    "    \"rating\": [\"1-2\", \"3\", \"4\", \"5\"],\n",
    "    \"in_stock\": [\"yes\", \"no\"]\n",
    "}\n",
    "\n",
    "ecommerce_query = \"Find me highly rated Apple laptops under $1000 that are in stock\"\n",
    "\n",
    "result = query_translation_to_filters(ecommerce_query, ecommerce_filters)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformations in LangChain and LlamaIndex\n",
    "\n",
    "## LangChain Query Transformations\n",
    "\n",
    "### 1. **Multi-Query Retrieval**\n",
    "- **Class**: `MultiQueryRetriever`\n",
    "- **Purpose**: Generates multiple variations of the user query from different perspectives\n",
    "- **Use Case**: Improves retrieval recall when queries are ambiguous or when single query can't cover complete information\n",
    "- **Implementation**: Automatically generates 3-5 alternative questions using LLM prompts\n",
    "\n",
    "### 2. **Query Decomposition**\n",
    "- **Module**: Query Analysis techniques\n",
    "- **Purpose**: Breaks down complex queries into distinct sub-questions\n",
    "- **Use Case**: When user input contains multiple questions that need separate retrieval\n",
    "- **Implementation**: Uses LLM function-calling to generate multiple sub-queries\n",
    "\n",
    "### 3. **Query Rewriting (Rewrite-Retrieve-Read)**\n",
    "- **Purpose**: Rewrites poorly phrased queries into better retrieval queries\n",
    "- **Use Case**: When original query is not optimal for embedding/retrieval\n",
    "- **Implementation**: Uses LLM to rephrase query before passing to retriever\n",
    "\n",
    "### 4. **Step-Back Prompting**\n",
    "- **Module**: `StepBackRetriever` (if available) or custom implementation\n",
    "- **Purpose**: Generates abstract, higher-level \"step back\" questions\n",
    "- **Use Case**: When specifics of a question trip up search quality\n",
    "- **Implementation**: First generates broader question, then queries using both original and step-back\n",
    "\n",
    "### 5. **HyDE (Hypothetical Document Embeddings)**\n",
    "- **Purpose**: Generates hypothetical answer/document first, then uses it for retrieval\n",
    "- **Use Case**: When query embeddings aren't similar to relevant document embeddings\n",
    "- **Implementation**: LLM generates hypothetical relevant document, uses that for similarity search\n",
    "\n",
    "### 6. **Query Routing**\n",
    "- **Purpose**: Routes queries to appropriate data sources/indexes\n",
    "- **Use Case**: Multiple indexes exist and only subset is relevant for given query\n",
    "- **Implementation**: LLM classifies query and selects appropriate retriever(s)\n",
    "\n",
    "### 7. **Query Structuring / Self-Query Retrieval**\n",
    "- **Class**: `SelfQueryRetriever`\n",
    "- **Purpose**: Converts natural language to structured queries with metadata filters\n",
    "- **Use Case**: When documents have searchable/filterable attributes\n",
    "- **Implementation**: \n",
    "  - Separates semantic query from metadata filters\n",
    "  - Translates to vector store-specific filter syntax\n",
    "  - Supports text-to-SQL, text-to-Cypher conversions\n",
    "\n",
    "### 8. **Conversational Query Transformation**\n",
    "- **Purpose**: Transforms conversation history into standalone search query\n",
    "- **Use Case**: Multi-turn conversations where context from history is needed\n",
    "- **Implementation**: Combines current query with conversation history\n",
    "\n",
    "### 9. **Query Expansion**\n",
    "- **Purpose**: Adds related terms, synonyms, or paraphrases to query\n",
    "- **Use Case**: When index is sensitive to query phrasing\n",
    "- **Implementation**: Generates expanded versions to increase retrieval chances\n",
    "\n",
    "### 10. **Reciprocal Rank Fusion (with Multi-Query)**\n",
    "- **Purpose**: Reorders documents retrieved from multiple query variants\n",
    "- **Use Case**: Combining results from multiple sub-queries effectively\n",
    "- **Implementation**: Uses RRF algorithm to merge and rank results\n",
    "\n",
    "---\n",
    "\n",
    "## LlamaIndex Query Transformations\n",
    "\n",
    "### 1. **HyDE Query Transform**\n",
    "- **Class**: `HyDEQueryTransform`\n",
    "- **Purpose**: Generates hypothetical document/answer, uses it for embedding\n",
    "- **Use Case**: When natural language queries don't embed well\n",
    "- **Implementation**: Wraps query engine with `TransformQueryEngine`\n",
    "- **Parameter**: `include_original` - whether to include original query\n",
    "\n",
    "### 2. **Multi-Step Query Decomposition**\n",
    "- **Class**: `StepDecomposeQueryTransform` + `MultiStepQueryEngine`\n",
    "- **Purpose**: Sequentially decomposes complex query into sub-questions\n",
    "- **Use Case**: Complex queries requiring iterative refinement against single knowledge source\n",
    "- **How it works**:\n",
    "  - Transforms query ‚Üí executes ‚Üí retrieves response\n",
    "  - Uses response + previous context to generate follow-up\n",
    "  - Continues until query is satisfied\n",
    "- **Difference from Sub-Question**: Sequential vs. parallel execution\n",
    "\n",
    "### 3. **Sub-Question Query Engine**\n",
    "- **Class**: `SubQuestionQueryEngine`\n",
    "- **Purpose**: Breaks complex query into multiple sub-questions executed in parallel\n",
    "- **Use Case**: \"Compare and contrast\" queries requiring multiple data sources\n",
    "- **How it works**:\n",
    "  - Generates sub-questions with target tools/indexes\n",
    "  - Executes all in parallel (or sequentially)\n",
    "  - Synthesizes responses into final answer\n",
    "- **Key Component**: `QuestionGenerator` to create sub-questions\n",
    "\n",
    "### 4. **Query Routing**\n",
    "\n",
    "#### a. **RouterQueryEngine**\n",
    "- **Purpose**: Routes to appropriate query engine based on query\n",
    "- **Selectors Available**:\n",
    "  - `LLMSingleSelector` - selects one query engine\n",
    "  - `LLMMultiSelector` - selects multiple query engines\n",
    "  - `PydanticSingleSelector` - uses OpenAI function calling (single)\n",
    "  - `PydanticMultiSelector` - uses OpenAI function calling (multiple)\n",
    "- **Use Case**: Different query engines for different query types (summarization vs. semantic search)\n",
    "\n",
    "#### b. **RouterRetriever**\n",
    "- **Purpose**: Routes to appropriate retriever based on query\n",
    "- **Same selector types as RouterQueryEngine**\n",
    "- **Use Case**: Multiple retrieval strategies (vector, keyword, etc.)\n",
    "\n",
    "#### c. **RetrieverRouterQueryEngine** (deprecated ‚Üí use ToolRetrieverRouterQueryEngine)\n",
    "- **Purpose**: Uses retriever to select query engines dynamically\n",
    "- **Use Case**: Large set of choices that need indexing\n",
    "\n",
    "#### d. **ToolRetrieverRouterQueryEngine**\n",
    "- **Purpose**: Retrieval-augmented routing when choices are too large\n",
    "- **How it works**: Indexes query engine tools, retrieves relevant ones, executes\n",
    "\n",
    "#### e. **SQL Router Query Engine**\n",
    "- **Purpose**: Routes between SQL query engine and vector query engines\n",
    "- **Use Case**: Hybrid queries needing both structured (SQL) and semantic search\n",
    "\n",
    "### 5. **Single-Step Query Decomposition**\n",
    "- **Purpose**: Transform query into single sub-question that's easier to answer\n",
    "- **Use Case**: Simplifying complex queries\n",
    "- **Implementation**: Part of broader decomposition framework\n",
    "\n",
    "### 6. **Query Rewriting/Expansion**\n",
    "- **Module**: Available in Query Transform Cookbook\n",
    "- **Purpose**: Rewrites queries for better retrieval\n",
    "- **Use Case**: Multiple phrasings of same query\n",
    "\n",
    "### 7. **Custom Query Transforms**\n",
    "- **Base Class**: `BaseQueryTransform`\n",
    "- **Purpose**: Create custom transformations\n",
    "- **Implementation**: Extend base class, implement transform logic\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences Between Frameworks\n",
    "\n",
    "| Feature | LangChain | LlamaIndex |\n",
    "|---------|-----------|------------|\n",
    "| **Multi-Query** | `MultiQueryRetriever` generates parallel queries | Sub-Question Query Engine generates parallel sub-queries |\n",
    "| **Sequential Decomposition** | Limited built-in support | `MultiStepQueryEngine` with sequential execution |\n",
    "| **Routing** | Query routing in query analysis | Extensive routing (Router Query Engine, Router Retriever) |\n",
    "| **HyDE** | Available but less emphasized | `HyDEQueryTransform` as core component |\n",
    "| **Self-Query** | `SelfQueryRetriever` with metadata filtering | Query structuring through routing |\n",
    "| **Composition** | Chain-based composition | Query engine composition with tools |\n",
    "| **Step-Back** | Explicitly mentioned in docs | Can be implemented with custom transforms |\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Patterns\n",
    "\n",
    "### LangChain Pattern\n",
    "```python\n",
    "# Multi-Query\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    num_queries=3\n",
    ")\n",
    "\n",
    "# Self-Query\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectorstore,\n",
    "    document_contents=\"description\",\n",
    "    metadata_field_info=metadata_info\n",
    ")\n",
    "```\n",
    "\n",
    "### LlamaIndex Pattern\n",
    "```python\n",
    "# HyDE\n",
    "from llama_index.core.indices.query.query_transform.base import HyDEQueryTransform\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "query_engine = TransformQueryEngine(base_engine, query_transform=hyde)\n",
    "\n",
    "# Multi-Step\n",
    "from llama_index.core.indices.query.query_transform.base import StepDecomposeQueryTransform\n",
    "step_decompose = StepDecomposeQueryTransform(llm=llm, verbose=True)\n",
    "query_engine = MultiStepQueryEngine(base_engine, query_transform=step_decompose)\n",
    "\n",
    "# Sub-Question\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=tools,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Routing\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import PydanticSingleSelector\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    query_engine_tools=tools\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**LangChain** focuses on:\n",
    "- Retriever-level transformations\n",
    "- Multi-query generation and fusion\n",
    "- Self-querying with metadata filtering\n",
    "- Integration with various retrievers\n",
    "\n",
    "**LlamaIndex** focuses on:\n",
    "- Query engine composition\n",
    "- Sequential and parallel decomposition\n",
    "- Extensive routing capabilities\n",
    "- Transform pipelines with custom components\n",
    "\n",
    "Both frameworks support the core query transformation patterns but with different implementation approaches and emphasis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--query-transformations)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
